{
  "best_metric": 2.1858036518096924,
  "best_model_checkpoint": "./results\\checkpoint-729",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 729,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0411522633744856,
      "grad_norm": 3.8521265983581543,
      "learning_rate": 4.931412894375857e-05,
      "loss": 4.8924,
      "step": 10
    },
    {
      "epoch": 0.0823045267489712,
      "grad_norm": 1.6995773315429688,
      "learning_rate": 4.862825788751715e-05,
      "loss": 3.8396,
      "step": 20
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 1.8235530853271484,
      "learning_rate": 4.794238683127572e-05,
      "loss": 3.6214,
      "step": 30
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 1.288438081741333,
      "learning_rate": 4.725651577503429e-05,
      "loss": 3.4163,
      "step": 40
    },
    {
      "epoch": 0.205761316872428,
      "grad_norm": 1.5939502716064453,
      "learning_rate": 4.657064471879287e-05,
      "loss": 3.2745,
      "step": 50
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 1.3326798677444458,
      "learning_rate": 4.5884773662551446e-05,
      "loss": 3.2004,
      "step": 60
    },
    {
      "epoch": 0.2880658436213992,
      "grad_norm": 1.2048859596252441,
      "learning_rate": 4.5198902606310016e-05,
      "loss": 3.1273,
      "step": 70
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 1.6720342636108398,
      "learning_rate": 4.451303155006859e-05,
      "loss": 2.9484,
      "step": 80
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 1.2196913957595825,
      "learning_rate": 4.3827160493827164e-05,
      "loss": 2.9477,
      "step": 90
    },
    {
      "epoch": 0.411522633744856,
      "grad_norm": 1.416808843612671,
      "learning_rate": 4.3141289437585735e-05,
      "loss": 2.9042,
      "step": 100
    },
    {
      "epoch": 0.45267489711934156,
      "grad_norm": 1.1883430480957031,
      "learning_rate": 4.2455418381344305e-05,
      "loss": 2.9056,
      "step": 110
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 1.4194164276123047,
      "learning_rate": 4.176954732510288e-05,
      "loss": 2.7841,
      "step": 120
    },
    {
      "epoch": 0.5349794238683128,
      "grad_norm": 1.431510329246521,
      "learning_rate": 4.108367626886145e-05,
      "loss": 2.8262,
      "step": 130
    },
    {
      "epoch": 0.5761316872427984,
      "grad_norm": 1.3164101839065552,
      "learning_rate": 4.039780521262003e-05,
      "loss": 2.8512,
      "step": 140
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 1.4199351072311401,
      "learning_rate": 3.971193415637861e-05,
      "loss": 2.7485,
      "step": 150
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 1.123904824256897,
      "learning_rate": 3.902606310013718e-05,
      "loss": 2.7493,
      "step": 160
    },
    {
      "epoch": 0.6995884773662552,
      "grad_norm": 1.19584321975708,
      "learning_rate": 3.834019204389575e-05,
      "loss": 2.8464,
      "step": 170
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.4791866540908813,
      "learning_rate": 3.7654320987654326e-05,
      "loss": 2.7099,
      "step": 180
    },
    {
      "epoch": 0.7818930041152263,
      "grad_norm": 1.362580418586731,
      "learning_rate": 3.6968449931412896e-05,
      "loss": 2.5292,
      "step": 190
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 1.4087636470794678,
      "learning_rate": 3.628257887517147e-05,
      "loss": 2.6836,
      "step": 200
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 1.7240546941757202,
      "learning_rate": 3.5596707818930044e-05,
      "loss": 2.4909,
      "step": 210
    },
    {
      "epoch": 0.9053497942386831,
      "grad_norm": 1.7315484285354614,
      "learning_rate": 3.4910836762688615e-05,
      "loss": 2.6687,
      "step": 220
    },
    {
      "epoch": 0.9465020576131687,
      "grad_norm": 1.436145305633545,
      "learning_rate": 3.4224965706447185e-05,
      "loss": 2.5876,
      "step": 230
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 1.2281118631362915,
      "learning_rate": 3.353909465020576e-05,
      "loss": 2.6089,
      "step": 240
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.332926034927368,
      "eval_runtime": 6.1148,
      "eval_samples_per_second": 79.316,
      "eval_steps_per_second": 9.976,
      "step": 243
    },
    {
      "epoch": 1.02880658436214,
      "grad_norm": 1.2619045972824097,
      "learning_rate": 3.285322359396434e-05,
      "loss": 2.575,
      "step": 250
    },
    {
      "epoch": 1.0699588477366255,
      "grad_norm": 1.2277798652648926,
      "learning_rate": 3.216735253772291e-05,
      "loss": 2.4894,
      "step": 260
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.9896219968795776,
      "learning_rate": 3.148148148148148e-05,
      "loss": 2.5347,
      "step": 270
    },
    {
      "epoch": 1.1522633744855968,
      "grad_norm": 1.3315385580062866,
      "learning_rate": 3.079561042524006e-05,
      "loss": 2.4904,
      "step": 280
    },
    {
      "epoch": 1.1934156378600824,
      "grad_norm": 1.4838976860046387,
      "learning_rate": 3.010973936899863e-05,
      "loss": 2.4777,
      "step": 290
    },
    {
      "epoch": 1.2345679012345678,
      "grad_norm": 1.0804013013839722,
      "learning_rate": 2.9423868312757202e-05,
      "loss": 2.498,
      "step": 300
    },
    {
      "epoch": 1.2757201646090535,
      "grad_norm": 2.31587290763855,
      "learning_rate": 2.8737997256515776e-05,
      "loss": 2.5538,
      "step": 310
    },
    {
      "epoch": 1.316872427983539,
      "grad_norm": 1.4902608394622803,
      "learning_rate": 2.8052126200274347e-05,
      "loss": 2.5696,
      "step": 320
    },
    {
      "epoch": 1.3580246913580247,
      "grad_norm": 1.8600517511367798,
      "learning_rate": 2.736625514403292e-05,
      "loss": 2.5781,
      "step": 330
    },
    {
      "epoch": 1.3991769547325104,
      "grad_norm": 1.3305636644363403,
      "learning_rate": 2.6680384087791498e-05,
      "loss": 2.4579,
      "step": 340
    },
    {
      "epoch": 1.4403292181069958,
      "grad_norm": 1.2206007242202759,
      "learning_rate": 2.5994513031550072e-05,
      "loss": 2.6137,
      "step": 350
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.48521089553833,
      "learning_rate": 2.5308641975308646e-05,
      "loss": 2.4809,
      "step": 360
    },
    {
      "epoch": 1.522633744855967,
      "grad_norm": 1.6602150201797485,
      "learning_rate": 2.4622770919067216e-05,
      "loss": 2.5406,
      "step": 370
    },
    {
      "epoch": 1.5637860082304527,
      "grad_norm": 1.3677750825881958,
      "learning_rate": 2.393689986282579e-05,
      "loss": 2.5966,
      "step": 380
    },
    {
      "epoch": 1.6049382716049383,
      "grad_norm": 1.1313345432281494,
      "learning_rate": 2.3251028806584364e-05,
      "loss": 2.484,
      "step": 390
    },
    {
      "epoch": 1.646090534979424,
      "grad_norm": 1.4281612634658813,
      "learning_rate": 2.2565157750342935e-05,
      "loss": 2.6169,
      "step": 400
    },
    {
      "epoch": 1.6872427983539096,
      "grad_norm": 1.1831413507461548,
      "learning_rate": 2.1879286694101512e-05,
      "loss": 2.4235,
      "step": 410
    },
    {
      "epoch": 1.7283950617283952,
      "grad_norm": 3.303074598312378,
      "learning_rate": 2.1193415637860082e-05,
      "loss": 2.2667,
      "step": 420
    },
    {
      "epoch": 1.7695473251028808,
      "grad_norm": 1.3070828914642334,
      "learning_rate": 2.0507544581618656e-05,
      "loss": 2.3717,
      "step": 430
    },
    {
      "epoch": 1.8106995884773662,
      "grad_norm": 1.3104411363601685,
      "learning_rate": 1.982167352537723e-05,
      "loss": 2.5381,
      "step": 440
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 4.932701587677002,
      "learning_rate": 1.91358024691358e-05,
      "loss": 2.426,
      "step": 450
    },
    {
      "epoch": 1.8930041152263375,
      "grad_norm": 1.3115006685256958,
      "learning_rate": 1.8449931412894378e-05,
      "loss": 2.4663,
      "step": 460
    },
    {
      "epoch": 1.934156378600823,
      "grad_norm": 6.259378433227539,
      "learning_rate": 1.7764060356652952e-05,
      "loss": 2.3301,
      "step": 470
    },
    {
      "epoch": 1.9753086419753085,
      "grad_norm": 1.2007378339767456,
      "learning_rate": 1.7078189300411522e-05,
      "loss": 2.424,
      "step": 480
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.2166941165924072,
      "eval_runtime": 4.761,
      "eval_samples_per_second": 101.869,
      "eval_steps_per_second": 12.812,
      "step": 486
    },
    {
      "epoch": 2.016460905349794,
      "grad_norm": 1.2344400882720947,
      "learning_rate": 1.6392318244170096e-05,
      "loss": 2.4174,
      "step": 490
    },
    {
      "epoch": 2.05761316872428,
      "grad_norm": 1.279353380203247,
      "learning_rate": 1.570644718792867e-05,
      "loss": 2.5077,
      "step": 500
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 1.121462106704712,
      "learning_rate": 1.5020576131687244e-05,
      "loss": 2.347,
      "step": 510
    },
    {
      "epoch": 2.139917695473251,
      "grad_norm": 1.3217642307281494,
      "learning_rate": 1.4334705075445818e-05,
      "loss": 2.4803,
      "step": 520
    },
    {
      "epoch": 2.1810699588477367,
      "grad_norm": 1.2339409589767456,
      "learning_rate": 1.364883401920439e-05,
      "loss": 2.4153,
      "step": 530
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.7242485284805298,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 2.4666,
      "step": 540
    },
    {
      "epoch": 2.263374485596708,
      "grad_norm": 1.1588057279586792,
      "learning_rate": 1.2277091906721536e-05,
      "loss": 2.3832,
      "step": 550
    },
    {
      "epoch": 2.3045267489711936,
      "grad_norm": 1.26125967502594,
      "learning_rate": 1.159122085048011e-05,
      "loss": 2.44,
      "step": 560
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 1.367698311805725,
      "learning_rate": 1.0905349794238684e-05,
      "loss": 2.4744,
      "step": 570
    },
    {
      "epoch": 2.386831275720165,
      "grad_norm": 1.4109995365142822,
      "learning_rate": 1.0219478737997256e-05,
      "loss": 2.3937,
      "step": 580
    },
    {
      "epoch": 2.42798353909465,
      "grad_norm": 1.1684609651565552,
      "learning_rate": 9.53360768175583e-06,
      "loss": 2.379,
      "step": 590
    },
    {
      "epoch": 2.4691358024691357,
      "grad_norm": 1.3638114929199219,
      "learning_rate": 8.847736625514404e-06,
      "loss": 2.3786,
      "step": 600
    },
    {
      "epoch": 2.5102880658436213,
      "grad_norm": 1.1845468282699585,
      "learning_rate": 8.161865569272976e-06,
      "loss": 2.3723,
      "step": 610
    },
    {
      "epoch": 2.551440329218107,
      "grad_norm": 1.5339399576187134,
      "learning_rate": 7.47599451303155e-06,
      "loss": 2.4927,
      "step": 620
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 1.3488985300064087,
      "learning_rate": 6.790123456790123e-06,
      "loss": 2.3217,
      "step": 630
    },
    {
      "epoch": 2.633744855967078,
      "grad_norm": 1.2930375337600708,
      "learning_rate": 6.104252400548697e-06,
      "loss": 2.3313,
      "step": 640
    },
    {
      "epoch": 2.674897119341564,
      "grad_norm": 1.1653492450714111,
      "learning_rate": 5.41838134430727e-06,
      "loss": 2.356,
      "step": 650
    },
    {
      "epoch": 2.7160493827160495,
      "grad_norm": 1.3128210306167603,
      "learning_rate": 4.732510288065844e-06,
      "loss": 2.408,
      "step": 660
    },
    {
      "epoch": 2.757201646090535,
      "grad_norm": 1.272587537765503,
      "learning_rate": 4.046639231824417e-06,
      "loss": 2.4714,
      "step": 670
    },
    {
      "epoch": 2.7983539094650207,
      "grad_norm": 1.1512579917907715,
      "learning_rate": 3.3607681755829907e-06,
      "loss": 2.3662,
      "step": 680
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 1.3763004541397095,
      "learning_rate": 2.6748971193415637e-06,
      "loss": 2.3168,
      "step": 690
    },
    {
      "epoch": 2.8806584362139915,
      "grad_norm": 1.5980695486068726,
      "learning_rate": 1.9890260631001372e-06,
      "loss": 2.3469,
      "step": 700
    },
    {
      "epoch": 2.9218106995884776,
      "grad_norm": 1.9622224569320679,
      "learning_rate": 1.3031550068587107e-06,
      "loss": 2.4428,
      "step": 710
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 1.1066951751708984,
      "learning_rate": 6.17283950617284e-07,
      "loss": 2.3675,
      "step": 720
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.1858036518096924,
      "eval_runtime": 4.7696,
      "eval_samples_per_second": 101.685,
      "eval_steps_per_second": 12.789,
      "step": 729
    }
  ],
  "logging_steps": 10,
  "max_steps": 729,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 787689284567040.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
